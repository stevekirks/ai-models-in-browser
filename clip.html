<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="icon" href="data:,">
    <title>CLIP - Image & Text Understanding in Browser</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        @keyframes fadeIn {
            from {
                opacity: 0;
                transform: translateY(10px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            padding: 20px;
        }

        .container {
            max-width: 1000px;
            margin: 0 auto;
            background: white;
            border-radius: 20px;
            padding: 40px;
            box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3);
        }

        h1 {
            color: #333;
            margin-bottom: 10px;
            font-size: 2.5em;
        }

        .subtitle {
            color: #666;
            margin-bottom: 30px;
            font-size: 1.1em;
        }

        .upload-section {
            margin-bottom: 20px;
            padding: 20px;
            background: #f8f9fa;
            border-radius: 10px;
        }

        .file-input-wrapper {
            display: inline-block;
            position: relative;
            overflow: hidden;
            margin-bottom: 20px;
        }

        .file-input-wrapper input[type=file] {
            position: absolute;
            left: -9999px;
        }

        .file-input-label {
            display: inline-block;
            padding: 12px 30px;
            background: #667eea;
            color: white;
            border-radius: 8px;
            cursor: pointer;
            font-weight: 600;
            transition: all 0.3s;
        }

        .file-input-label:hover {
            background: #5568d3;
            transform: translateY(-2px);
        }

        .image-preview {
            margin-top: 20px;
            display: flex;
            flex-wrap: wrap;
            gap: 15px;
            justify-content: center;
        }

        .image-item {
            position: relative;
            text-align: center;
        }

        .image-item img {
            max-width: 250px;
            max-height: 250px;
            border-radius: 10px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            cursor: pointer;
            transition: transform 0.2s;
        }

        .image-item img:hover {
            transform: scale(1.05);
        }

        .remove-btn {
            position: absolute;
            top: 5px;
            right: 5px;
            background: rgba(255, 0, 0, 0.8);
            color: white;
            border: none;
            border-radius: 50%;
            width: 25px;
            height: 25px;
            cursor: pointer;
            font-weight: bold;
            line-height: 1;
        }

        .remove-btn:hover {
            background: rgba(255, 0, 0, 1);
        }

        .query-section {
            margin-bottom: 30px;
        }

        .query-section h2 {
            color: #333;
            margin-bottom: 15px;
            font-size: 1.5em;
        }

        .text-queries {
            display: flex;
            flex-direction: column;
            gap: 10px;
            margin-bottom: 20px;
        }

        .text-queries input {
            padding: 12px;
            border: 2px solid #e0e0e0;
            border-radius: 8px;
            font-size: 16px;
            transition: border-color 0.3s;
        }

        .text-queries input:focus {
            outline: none;
            border-color: #667eea;
        }

        .quick-examples {
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
            margin-bottom: 20px;
        }

        .example-btn {
            padding: 8px 16px;
            background: #e0e7ff;
            color: #667eea;
            border: none;
            border-radius: 6px;
            cursor: pointer;
            font-size: 14px;
            transition: all 0.3s;
        }

        .example-btn:hover {
            background: #667eea;
            color: white;
        }

        .analyze-btn {
            width: 100%;
            padding: 15px;
            background: #764ba2;
            color: white;
            border: none;
            border-radius: 8px;
            font-size: 18px;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s;
        }

        .analyze-btn:hover:not(:disabled) {
            background: #663399;
            transform: translateY(-2px);
        }

        .analyze-btn:disabled {
            background: #ccc;
            cursor: not-allowed;
        }

        .results-section {
            margin-top: 30px;
            padding: 20px;
            background: #f8f9fa;
            border-radius: 10px;
        }

        .results-section h2 {
            color: #333;
            margin-bottom: 15px;
        }

        .result-item {
            padding: 15px;
            background: white;
            border-radius: 8px;
            margin-bottom: 10px;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        }

        .result-text {
            font-weight: 600;
            color: #333;
            margin-bottom: 8px;
        }

        .similarity-bar {
            width: 100%;
            height: 30px;
            background: #e0e0e0;
            border-radius: 15px;
            overflow: hidden;
            position: relative;
        }

        .similarity-fill {
            height: 100%;
            background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
            transition: width 0.5s ease;
            display: flex;
            align-items: center;
            justify-content: flex-end;
            padding-right: 10px;
        }

        .similarity-score {
            color: white;
            font-weight: 600;
            font-size: 14px;
        }

        .status {
            padding: 15px;
            border-radius: 8px;
            margin-bottom: 20px;
            text-align: center;
            font-weight: 600;
        }

        .status.loading {
            background: #fff3cd;
            color: #856404;
        }

        .status.ready {
            background: #d4edda;
            color: #155724;
        }

        .status.error {
            background: #f8d7da;
            color: #721c24;
        }

        .hidden {
            display: none;
        }

        .progress-container {
            margin: 20px 0;
            padding: 20px;
            background: #f8f9fa;
            border-radius: 10px;
        }

        .progress-bar-wrapper {
            width: 100%;
            height: 30px;
            background: #e0e0e0;
            border-radius: 15px;
            overflow: hidden;
            margin-bottom: 10px;
        }

        .progress-bar-fill {
            height: 100%;
            background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
            transition: width 0.3s ease;
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-weight: 600;
            font-size: 14px;
        }

        .progress-text {
            text-align: center;
            color: #666;
            font-size: 14px;
        }

        .info-box {
            background: #e3f2fd;
            border-left: 4px solid #2196f3;
            padding: 15px;
            border-radius: 6px;
            margin-bottom: 20px;
        }

        .info-box p {
            color: #1976d2;
            margin-bottom: 5px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>ü§ñ CLIP in Browser</h1>
        <p class="subtitle">Vision-Language AI Model - Image & Text Understanding</p>

        <div class="info-box">
            <p><strong>What is CLIP?</strong></p>
            <p>A lightweight AI model that understands the relationship between images and text. Upload images and provide text descriptions to see how well they match!</p>
            <p style="font-size: 0.9em; margin-top: 10px;">Uses <a href="https://github.com/openai/CLIP" target="_blank" style="color: #1976d2;">CLIP by OpenAI</a> via <a href="https://huggingface.co/Xenova/clip-vit-base-patch32" target="_blank" style="color: #1976d2;">Hugging Face</a> and <a href="https://github.com/xenova/transformers.js" target="_blank" style="color: #1976d2;">Transformers.js</a></p>
        </div>

        <div class="upload-section" id="modelUploadSection" style="display: none;">
            <h2>üì¶ Upload Model Files</h2>
            
            <div class="info-box" style="background: #fff3cd; border-left-color: #ffc107;">
                <p>HuggingFace's CORS policy prevents browsers from loading model files directly from their CDN, so there's a manual step to get the files.</p>
                
                <p style="margin-top: 15px;"><strong>How to get model files:</strong></p>
                <ol style="margin: 10px 0; padding-left: 20px; color: #856404;">
                    <li>Visit <a href="https://huggingface.co/Xenova/clip-vit-base-patch32" target="_blank" style="color: #856404; text-decoration: underline;">HuggingFace CLIP Model</a> (or alternatives like <code>clip-vit-base-patch16</code>, <code>clip-vit-large-patch14</code>)</li>
                    <li>Download these files to a folder on your computer:
                        <ul style="margin: 5px 0 5px 20px; font-size: 0.9em;">
                            <li><code>config.json</code></li>
                            <li><code>preprocessor_config.json</code></li>
                            <li><code>tokenizer_config.json</code></li>
                            <li><code>tokenizer.json</code></li>
                            <li><code>vocab.json</code></li>
                            <li><code>merges.txt</code></li>
                            <li><code>onnx/model_quantized.onnx</code> (152 MB)</li>
                        </ul>
                    </li>
                    <li>Click "Select Folder" below and choose your model folder</li>
                </ol>
                <p style="font-size: 0.85em; margin-top: 10px;"><strong>Tip:</strong> Keep the folder structure (files in root, onnx file in "onnx" subfolder)</p>
            </div>
            
            <div style="margin: 20px 0;">
                <input type="file" id="modelFiles" webkitdirectory directory multiple style="margin-bottom: 10px;">
                <div id="uploadedFiles" style="font-size: 0.85em; color: #666; max-height: 100px; overflow-y: auto; margin-top: 10px;"></div>
            </div>
            
            <button id="loadModelBtn" class="analyze-btn" onclick="loadModel()">
                Load Model
            </button>
        </div>

        <div id="status" class="status loading">
            Select and load a model to begin...
        </div>

        <div class="upload-section" style="margin-top: 10px;">
            <h2>üì∏ Upload Images</h2>
            <div class="file-input-wrapper">
                <input type="file" id="imageInput" accept="image/*" multiple>
                <label for="imageInput" class="file-input-label">Choose Images</label>
            </div>
            <div id="imagePreview" class="image-preview hidden"></div>
        </div>

        <div class="query-section">
            <h2>üìù Enter Text Descriptions</h2>
            <p style="color: #666; margin-bottom: 15px;">Enter at least 2 text descriptions to compare with your images:</p>
            
            <div class="quick-examples">
                <button class="example-btn" onclick="loadExample('animal')">Animal Examples</button>
                <button class="example-btn" onclick="loadExample('scene')">Scene Examples</button>
                <button class="example-btn" onclick="loadExample('object')">Object Examples</button>
                <button class="example-btn" onclick="addTextInput()">+ Add More</button>
                <button class="example-btn" onclick="clearTexts()">Clear All</button>
            </div>

            <div id="textQueriesContainer" class="text-queries">
                <input type="text" class="text-input" data-index="0" placeholder="Text description 1 (e.g., 'a photo of a dog')">
                <input type="text" class="text-input" data-index="1" placeholder="Text description 2 (e.g., 'a photo of a cat')">
            </div>

            <button id="analyzeBtn" class="analyze-btn" disabled onclick="analyzeImage()">
                Analyze Images & Text Similarity
            </button>
        </div>

        <div id="progressContainer" class="progress-container hidden">
            <div class="progress-bar-wrapper">
                <div id="progressBar" class="progress-bar-fill" style="width: 0%">0%</div>
            </div>
            <div id="progressText" class="progress-text">Processing images...</div>
        </div>

        <div id="results" class="results-section hidden">
            <h2>üìä Results - Text Similarity Scores</h2>
            <div id="resultsContent"></div>
        </div>
    </div>

    <script type="module">
        import { pipeline, env } from 'https://cdn.jsdelivr.net/npm/@huggingface/transformers@3.0.2';

        // Configure environment to use jsdelivr CDN (has proper CORS)
        env.backends.onnx.wasm.numThreads = 1;
        env.useBrowserCache = true;
        env.allowLocalModels = true;
        env.allowRemoteModels = true;
        
        // Try to use jsdelivr as CDN for models (better CORS support)
        env.remoteHost = 'https://cdn.jsdelivr.net/npm/@huggingface/transformers@3.0.2/dist/';
        env.remotePathTemplate = 'models/{model}/';

        let clipModel = null;
        let uploadedImages = [];
        let uploadedModelFiles = {};
        let modelFileMap = {};
        
        // Set up global fetch override for uploaded files
        const originalFetch = window.fetch;
        window.fetch = async function(url, options) {
            const urlStr = url.toString();
            
            // Only intercept if we have uploaded files
            if (Object.keys(modelFileMap).length > 0) {
                // Try multiple ways to extract the path
                const pathVariants = [];
                
                // 1. Path after 'models/'
                if (urlStr.includes('models/')) {
                    const afterModels = urlStr.split('models/')[1].split('?')[0];
                    pathVariants.push(afterModels);
                    // Also try without the first folder name
                    if (afterModels.includes('/')) {
                        pathVariants.push(afterModels.split('/').slice(1).join('/'));
                    }
                }
                
                // 2. Just the filename
                pathVariants.push(urlStr.split('/').pop().split('?')[0]);
                
                // Try each path variant
                for (const requestedPath of pathVariants) {
                    if (modelFileMap[requestedPath]) {
                        const file = modelFileMap[requestedPath];
                        const content = await file.arrayBuffer();
                        
                        return new Response(content, {
                            status: 200,
                            headers: {
                                'Content-Type': requestedPath.endsWith('.json') ? 'application/json' : 'application/octet-stream',
                                'Content-Length': content.byteLength
                            }
                        });
                    }
                }
            }
            
            // Use original fetch
            return originalFetch(url, options);
        };
        
        // Initialize on page load
        document.addEventListener('DOMContentLoaded', function() {
            // Try to load from local /models folder first
            tryLoadLocalModels();
            
            // Handle file upload
            document.getElementById('modelFiles').addEventListener('change', function(e) {
                const files = Array.from(e.target.files);
                uploadedModelFiles = {};
                
                files.forEach(file => {
                    const path = file.webkitRelativePath || file.name;
                    uploadedModelFiles[path] = file;
                });
                
                // Check for required files
                const hasOnnx = files.some(f => f.name.includes('.onnx'));
                const hasConfig = files.some(f => f.name === 'config.json');
                const hasPreprocessor = files.some(f => f.name === 'preprocessor_config.json');
                
                const fileList = document.getElementById('uploadedFiles');
                let statusHtml = `<strong>‚úì ${files.length} files selected</strong><br>`;
                
                if (!hasOnnx) statusHtml += '<span style="color: red;">‚ö† Missing .onnx file!</span><br>';
                if (!hasConfig) statusHtml += '<span style="color: red;">‚ö† Missing config.json!</span><br>';
                if (!hasPreprocessor) statusHtml += '<span style="color: red;">‚ö† Missing preprocessor_config.json!</span><br>';
                
                statusHtml += files.slice(0, 5).map(f => f.webkitRelativePath || f.name).join('<br>') +
                    (files.length > 5 ? '<br>...' : '');
                    
                fileList.innerHTML = statusHtml;
            });
        });
        
        // Check if model files exist in browser cache and extract model name
        async function checkCachedModel() {
            try {
                const cache = await caches.open('transformers-cache');
                const keys = await cache.keys();
                
                // Look for model files in cache and extract the model path
                for (const req of keys) {
                    const url = req.url;
                    if (url.includes('config.json') && url.includes('/models/')) {
                        // Extract model name from URL like "/models/models_man/config.json"
                        const match = url.match(/\/models\/([^\/]+)\//);
                        if (match) {
                            const modelName = match[1];
                            // Verify we also have the ONNX file
                            const hasOnnx = keys.some(r => r.url.includes(modelName) && r.url.includes('model_quantized.onnx'));
                            if (hasOnnx) {
                                return modelName;
                            }
                        }
                    }
                }
                return null;
            } catch (e) {
                return null;
            }
        }

        // Try loading from /models folder or cache
        async function tryLoadLocalModels() {
            const statusEl = document.getElementById('status');
            
            try {
                statusEl.textContent = 'Checking for local model files...';
                
                env.allowLocalModels = true;
                env.allowRemoteModels = false;
                env.localModelPath = './models/';
                
                clipModel = await pipeline('zero-shot-image-classification', 
                    'clip-vit-base-patch32');
                
                statusEl.className = 'status ready';
                statusEl.textContent = '‚úÖ Model loaded from local files! Upload images to start.';
                
            } catch (error) {
                // Check if we have cached uploaded files
                statusEl.textContent = 'Checking cache...';
                const cachedModelName = await checkCachedModel();
                
                if (cachedModelName) {
                    // Try loading from cache
                    try {
                        statusEl.textContent = 'Loading from cached files...';
                        
                        env.allowLocalModels = true;
                        env.allowRemoteModels = false;
                        env.localModelPath = './';
                        env.backends.onnx.wasm.proxy = false;
                        
                        clipModel = await pipeline('zero-shot-image-classification', `models/${cachedModelName}`);
                        
                        statusEl.className = 'status ready';
                        statusEl.textContent = '‚úÖ Model loaded from cache! Upload images to start.';
                        return;
                    } catch (cacheError) {
                        console.error('Cache load failed:', cacheError);
                        // Cache load failed, fall through to upload UI
                    }
                }
                
                // No local files or cache, show upload option
                statusEl.className = 'status loading';
                statusEl.textContent = 'Model files not found locally. Please upload model files below.';
                document.getElementById('modelUploadSection').style.display = 'block';
            }
        }

        // Load the model from uploaded files
        window.loadModel = async function() {
            const statusEl = document.getElementById('status');
            const loadBtn = document.getElementById('loadModelBtn');
            
            try {
                loadBtn.disabled = true;
                loadBtn.textContent = 'Loading...';
                clipModel = null;
                
                // Check if files were uploaded
                if (Object.keys(uploadedModelFiles).length === 0) {
                    throw new Error('Please select model files first');
                }
                
                statusEl.className = 'status loading';
                statusEl.textContent = 'Processing uploaded files...';
                
                // Get the base folder name (e.g., "clip-vit-base-patch32")
                const firstFile = Object.keys(uploadedModelFiles)[0];
                const baseFolderName = firstFile.split('/')[0];
                
                // Populate global file map for fetch interception
                modelFileMap = {};
                for (const [path, file] of Object.entries(uploadedModelFiles)) {
                    const parts = path.split('/');
                    const relativePath = parts.slice(1).join('/'); // Remove base folder
                    const filename = parts[parts.length - 1]; // Just the filename
                    
                    // Store by multiple keys for flexible matching
                    modelFileMap[relativePath] = file;
                    modelFileMap[filename] = file;
                    modelFileMap[`${baseFolderName}/${relativePath}`] = file;
                }
                
                statusEl.textContent = 'Loading model from uploaded files...';
                
                env.allowLocalModels = true;
                env.allowRemoteModels = false;
                env.localModelPath = './';
                env.backends.onnx.wasm.proxy = false;
                
                // Use the model path that matches our fetch intercept
                clipModel = await pipeline('zero-shot-image-classification', `models/${baseFolderName}`);
                
                statusEl.className = 'status ready';
                statusEl.textContent = '‚úÖ Model loaded from uploaded files! Upload images to start.';
                
                // Hide upload section after successful load
                document.getElementById('modelUploadSection').style.display = 'none';
                
                loadBtn.disabled = false;
                loadBtn.textContent = 'Load Model';
                
            } catch (error) {
                console.error('Error loading model:', error);
                statusEl.className = 'status error';
                statusEl.innerHTML = `‚ùå Could not load model<br><small>${error.message}</small>`;
                loadBtn.disabled = false;
                loadBtn.textContent = 'Load Model';
            }
        };

        // Handle image upload
        document.getElementById('imageInput').addEventListener('change', function(e) {
            const files = Array.from(e.target.files);
            if (files.length === 0) return;

            files.forEach(file => {
                const reader = new FileReader();
                reader.onload = function(event) {
                    const imageData = event.target.result;
                    uploadedImages.push(imageData);
                    
                    const preview = document.getElementById('imagePreview');
                    const imageItem = document.createElement('div');
                    imageItem.className = 'image-item';
                    imageItem.innerHTML = `
                        <button class="remove-btn" onclick="removeImage(${uploadedImages.length - 1})">√ó</button>
                        <img src="${imageData}" alt="Uploaded image ${uploadedImages.length}">
                    `;
                    preview.appendChild(imageItem);
                    preview.classList.remove('hidden');
                    
                    // Enable analyze button if model is loaded
                    if (clipModel) {
                        document.getElementById('analyzeBtn').disabled = false;
                    }
                };
                reader.readAsDataURL(file);
            });
        });

        // Remove image
        window.removeImage = function(index) {
            uploadedImages.splice(index, 1);
            const preview = document.getElementById('imagePreview');
            preview.innerHTML = '';
            
            uploadedImages.forEach((imageData, i) => {
                const imageItem = document.createElement('div');
                imageItem.className = 'image-item';
                imageItem.innerHTML = `
                    <button class="remove-btn" onclick="removeImage(${i})">√ó</button>
                    <img src="${imageData}" alt="Uploaded image ${i + 1}">
                `;
                preview.appendChild(imageItem);
            });
            
            if (uploadedImages.length === 0) {
                preview.classList.add('hidden');
                document.getElementById('analyzeBtn').disabled = true;
                document.getElementById('results').classList.add('hidden');
            }
        };

        // Analyze image
        window.analyzeImage = async function() {
            if (uploadedImages.length === 0 || !clipModel) {
                alert('Please wait for the model to load and upload at least one image first.');
                return;
            }

            // Get text inputs
            const texts = [];
            document.querySelectorAll('.text-input').forEach(input => {
                const text = input.value.trim();
                if (text) texts.push(text);
            });

            if (texts.length < 2) {
                alert('Please enter at least 2 text descriptions for comparison.');
                return;
            }

            try {
                const analyzeBtn = document.getElementById('analyzeBtn');
                analyzeBtn.disabled = true;
                analyzeBtn.textContent = 'Analyzing...';

                // Show progress bar and results section
                const progressContainer = document.getElementById('progressContainer');
                const progressBar = document.getElementById('progressBar');
                const progressText = document.getElementById('progressText');
                const resultsSection = document.getElementById('results');
                const resultsContent = document.getElementById('resultsContent');
                
                progressContainer.classList.remove('hidden');
                resultsSection.classList.remove('hidden');
                resultsContent.innerHTML = '';

                const totalImages = uploadedImages.length;

                // Process each image sequentially
                for (let i = 0; i < totalImages; i++) {
                    // Update progress
                    const progress = Math.round(((i) / totalImages) * 100);
                    progressBar.style.width = `${progress}%`;
                    progressBar.textContent = `${progress}%`;
                    progressText.textContent = `Processing image ${i + 1} of ${totalImages}...`;

                    // Run classification - pass the image URL directly
                    const results = await clipModel(uploadedImages[i], texts);
                    
                    // Display this image's results immediately
                    displaySingleResult(resultsContent, i, results);

                    // Update progress to show completion
                    const newProgress = Math.round(((i + 1) / totalImages) * 100);
                    progressBar.style.width = `${newProgress}%`;
                    progressBar.textContent = `${newProgress}%`;
                }
                
                // Final progress update
                progressText.textContent = `‚úÖ Completed ${totalImages} image${totalImages > 1 ? 's' : ''}!`;
                
                // Hide progress bar after 2 seconds
                setTimeout(() => {
                    progressContainer.classList.add('hidden');
                }, 2000);

                analyzeBtn.disabled = false;
                analyzeBtn.textContent = 'Analyze Images & Text Similarity';
            } catch (error) {
                console.error('Error during analysis:', error);
                alert('Error during analysis: ' + error.message);
                const analyzeBtn = document.getElementById('analyzeBtn');
                const progressContainer = document.getElementById('progressContainer');
                progressContainer.classList.add('hidden');
                analyzeBtn.disabled = false;
                analyzeBtn.textContent = 'Analyze Images & Text Similarity';
            }
        };

        // Display single result
        function displaySingleResult(resultsContent, imageIndex, results) {
            const imageSection = document.createElement('div');
            imageSection.style.marginBottom = '30px';
            imageSection.style.padding = '20px';
            imageSection.style.background = 'white';
            imageSection.style.borderRadius = '10px';
            imageSection.style.animation = 'fadeIn 0.3s ease-in';
            
            // Add image preview
            const imagePreview = document.createElement('div');
            imagePreview.style.textAlign = 'center';
            imagePreview.style.marginBottom = '15px';
            imagePreview.innerHTML = `
                <img src="${uploadedImages[imageIndex]}" 
                     alt="Image ${imageIndex + 1}" 
                     style="max-width: 200px; max-height: 200px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
                <h3 style="color: #333; margin-top: 10px;">Image ${imageIndex + 1} Results</h3>
            `;
            imageSection.appendChild(imagePreview);
            
            // Sort by score descending
            results.sort((a, b) => b.score - a.score);

            // Filter results: always show top result, then only those above 30%
            const filteredResults = results.filter((result, index) => {
                const percentage = result.score * 100;
                return index === 0 || (percentage >= 30 && index < 3);
            });

            filteredResults.forEach(result => {
                const percentage = (result.score * 100).toFixed(1);
                const item = document.createElement('div');
                item.className = 'result-item';
                item.innerHTML = `
                    <div class="result-text">${result.label}</div>
                    <div class="similarity-bar">
                        <div class="similarity-fill" style="width: ${percentage}%">
                            <span class="similarity-score">${percentage}%</span>
                        </div>
                    </div>
                `;
                imageSection.appendChild(item);
            });
            
            resultsContent.appendChild(imageSection);
        }

        // Example presets
        window.loadExample = function(type) {
            clearTexts();
            const examples = {
                animal: [
                    'a photo of a dog',
                    'a photo of a cat',
                    'a photo of a bird',
                    'a photo of a horse',
                    'a photo of a fish'
                ],
                scene: [
                    'a photo of a beach',
                    'a photo of a mountain',
                    'a photo of a city street',
                    'a photo of a forest',
                    'a photo of a desert'
                ],
                object: [
                    'a photo of a car',
                    'a photo of a bicycle',
                    'a photo of furniture',
                    'a photo of food',
                    'a photo of electronics'
                ]
            };

            const texts = examples[type] || [];
            
            // Add more inputs if needed
            const container = document.getElementById('textQueriesContainer');
            while (container.children.length < texts.length) {
                addTextInput(false);
            }
            
            // Fill in the examples
            const inputs = container.querySelectorAll('.text-input');
            texts.forEach((text, index) => {
                if (inputs[index]) {
                    inputs[index].value = text;
                    saveText(index, text);
                }
            });
            
            saveTextCount();
        };

        window.clearTexts = function() {
            // Clear all text inputs
            document.querySelectorAll('.text-input').forEach((input, index) => {
                input.value = '';
                localStorage.removeItem(`clipText${index}`);
            });
            
            // Reset to 2 inputs
            const container = document.getElementById('textQueriesContainer');
            while (container.children.length > 2) {
                container.removeChild(container.lastChild);
            }
            textInputCount = 2;
            saveTextCount();
            attachTextInputListeners();
        };

        let textInputCount = 2;

        // Load saved text descriptions from localStorage
        function loadSavedTexts() {
            const savedCount = parseInt(localStorage.getItem('clipTextCount')) || 2;
            const container = document.getElementById('textQueriesContainer');
            
            // Clear existing inputs except the first 2
            while (container.children.length > savedCount) {
                container.removeChild(container.lastChild);
            }
            
            // Add inputs if needed
            while (container.children.length < savedCount) {
                addTextInput(false);
            }
            
            // Load saved values
            const inputs = container.querySelectorAll('.text-input');
            inputs.forEach((input, index) => {
                const savedText = localStorage.getItem(`clipText${index}`);
                if (savedText) {
                    input.value = savedText;
                }
            });
            
            textInputCount = savedCount;
        }

        // Save text description to localStorage
        function saveText(index, value) {
            localStorage.setItem(`clipText${index}`, value);
        }

        // Save text count
        function saveTextCount() {
            localStorage.setItem('clipTextCount', textInputCount);
        }

        // Add event listeners to text inputs
        function attachTextInputListeners() {
            document.querySelectorAll('.text-input').forEach((input, index) => {
                input.removeEventListener('input', handleTextInput);
                input.addEventListener('input', handleTextInput);
                input.dataset.index = index;
            });
        }

        function handleTextInput(e) {
            const index = parseInt(e.target.dataset.index);
            saveText(index, e.target.value);
        }

        // Add new text input
        window.addTextInput = function(save = true) {
            const container = document.getElementById('textQueriesContainer');
            const input = document.createElement('input');
            input.type = 'text';
            input.className = 'text-input';
            input.dataset.index = textInputCount;
            input.placeholder = `Text description ${textInputCount + 1}`;
            container.appendChild(input);
            textInputCount++;
            
            if (save) {
                saveTextCount();
            }
            attachTextInputListeners();
        };

        // Initial setup
        attachTextInputListeners();

        // Initialize
        loadSavedTexts();
        // Model is loaded on demand via button
    </script>
</body>
</html>
